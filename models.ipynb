{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "     \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        mask = self.mask.to(device=device)\n",
    "        \n",
    "        x = x*mask\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CNSteerableLeNet(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, imsize, kernel_size=5, N=8):\n",
    "        super(CNSteerableLeNet, self).__init__()\n",
    "        \n",
    "        z = 0.5*(imsize - 2)\n",
    "        z = int(0.5*(z - 2))\n",
    "        \n",
    "        self.r2_act = gspaces.Rot2dOnR2(N)\n",
    "        \n",
    "        in_type = e2nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        out_type = e2nn.FieldType(self.r2_act, 6*[self.r2_act.regular_repr])\n",
    "        self.mask = e2nn.MaskModule(in_type, imsize, margin=1)\n",
    "        self.conv1 = e2nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu1 = e2nn.ReLU(out_type, inplace=True)\n",
    "        self.pool1 = e2nn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "\n",
    "        in_type = self.pool1.out_type\n",
    "        out_type = e2nn.FieldType(self.r2_act, 16*[self.r2_act.regular_repr])\n",
    "        self.conv2 = e2nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu2 = e2nn.ReLU(out_type, inplace=True)\n",
    "        self.pool2 = e2nn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "        \n",
    "        self.gpool = e2nn.GroupPooling(out_type)\n",
    "\n",
    "        self.fc1   = nn.Linear(16*z*z, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, out_chan)\n",
    "        \n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # dummy parameter for tracking device\n",
    "        self.dummy = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "        \n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "     \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "        return\n",
    "      \n",
    "      \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = e2nn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.gpool(x)\n",
    "        x = x.tensor\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class DNSteerableLeNet(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, imsize, kernel_size=5, N=8):\n",
    "        super(DNSteerableLeNet, self).__init__()\n",
    "        \n",
    "        z = 0.5*(imsize - 2)\n",
    "        z = int(0.5*(z - 2))\n",
    "        \n",
    "        self.r2_act = gspaces.FlipRot2dOnR2(N)\n",
    "        \n",
    "        in_type = e2nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        out_type = e2nn.FieldType(self.r2_act, 6*[self.r2_act.regular_repr])\n",
    "        self.mask = e2nn.MaskModule(in_type, imsize, margin=1)\n",
    "        self.conv1 = e2nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu1 = e2nn.ReLU(out_type, inplace=True)\n",
    "        self.pool1 = e2nn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "\n",
    "        in_type = self.pool1.out_type\n",
    "        out_type = e2nn.FieldType(self.r2_act, 16*[self.r2_act.regular_repr])\n",
    "        self.conv2 = e2nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu2 = e2nn.ReLU(out_type, inplace=True)\n",
    "        self.pool2 = e2nn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "        \n",
    "        self.gpool = e2nn.GroupPooling(out_type)\n",
    "\n",
    "        self.fc1   = nn.Linear(16*z*z, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, out_chan)\n",
    "        \n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # dummy parameter for tracking device\n",
    "        self.dummy = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "     \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "        return\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = e2nn.GeometricTensor(x, self.input_type)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.gpool(x)\n",
    "        x = x.tensor\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class DNRestrictedLeNet(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, imsize, kernel_size=5, N=8):\n",
    "        super(DNRestrictedLeNet, self).__init__()\n",
    "        \n",
    "        z = 0.5*(imsize - 2)\n",
    "        z = int(0.5*(z - 2))\n",
    "        \n",
    "        self.r2_act = gspaces.FlipRot2dOnR2(N)\n",
    "        \n",
    "        in_type = e2nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        out_type = e2nn.FieldType(self.r2_act, 6*[self.r2_act.regular_repr])\n",
    "        self.mask = e2nn.MaskModule(in_type, imsize, margin=1)\n",
    "        self.conv1 = e2nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu1 = e2nn.ReLU(out_type, inplace=True)\n",
    "        self.pool1 = e2nn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "\n",
    "        self.gpool = e2nn.GroupPooling(out_type)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size, padding=1)\n",
    "        \n",
    "        self.fc1   = nn.Linear(16*z*z, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, out_chan)\n",
    "        \n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # dummy parameter for tracking device\n",
    "        self.dummy = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "     \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "        return\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = e2nn.GeometricTensor(x, self.input_type)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.gpool(x)\n",
    "        x = x.tensor\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HMTNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "        This network has been taken directly from \"transfer learning for radio galaxy classification\"\n",
    "        https://arxiv.org/abs/1903.11921\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, in_chan, out_chan, imsize, kernel_size=11, N=None):\n",
    "        super(HMTNet,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_chan,out_channels=6,kernel_size=(11,11),padding=5,stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=(5,5),padding=2,stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=24,kernel_size=(3,3),padding=1,stride=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24,out_channels=24,kernel_size=(3,3),padding=1,stride=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=24,out_channels=16,kernel_size=(3,3),padding=1,stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=5, stride=5)\n",
    "        self.bnorm1 = nn.BatchNorm2d(6)\n",
    "        self.bnorm2 = nn.BatchNorm2d(16)\n",
    "        self.bnorm3 = nn.BatchNorm2d(24)\n",
    "        self.bnorm4 = nn.BatchNorm2d(24)\n",
    "        self.bnorm5 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.fc1 = nn.Linear(400,256) #channel_size * width * height\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc3 = nn.Linear(256,out_chan)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # dummy parameter for tracking device\n",
    "        self.dummy = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "\n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x  = F.relu(self.conv1(x))\n",
    "        x = self.bnorm1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x  = F.relu(self.conv2(x))\n",
    "        x = self.bnorm2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x  = F.relu(self.conv3(x))\n",
    "        x = self.bnorm3(x)\n",
    "        \n",
    "        x  = F.relu(self.conv4(x))\n",
    "        x = self.bnorm4(x)\n",
    "        \n",
    "        x  = F.relu(self.conv5(x))\n",
    "        x = self.bnorm5(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        v = self.fc3(x)\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
